{
  "hash": "93c67fc409d2c3f3c243ff2dcbe410fb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Normalizing your data and PCA\"\nformat: html\n---\n\n# Introduction\n\nThis chapter demonstrates basic unsupervised machine learning concepts using Python.\n\n::: {.callout-tip}\n## Learning Objectives\n\n- Understand the difference between supervised and unsupervised learning.\n- Apply PCA and clustering to example data.\n- Visualize results.\n:::\n<!-- end callout -->\n\n\n## Normalization (Z-score Standardization)\n\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\n\nThe formula for Z-score is:\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\nWhere:\n- $x$ is the original data point.\n- $\\mu$ is the mean of the data.\n- $\\sigma$ is the standard deviation of the data.\n\nFor example, say you have two variables or *features* on very different scales. \n\n\n| Age | Weight (grams) |\n|-----|------------|\n| 25  | 65000      |\n| 30  | 70000      |\n| 35  | 75000      |\n| 40  | 80000      |\n| 45  | 85000      |\n| 50  | 90000      |\n| 55  | 95000      |\n| 60  | 100000     |\n| 65  | 105000     |\n| 70  | 110000     |\n| 75  | 115000     |\n| 80  | 120000     |\n\nIf these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.\n\nHence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.\n\n::: {#46cf961d .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal data:\nAge: mean=43.6, std=13.1\nWeight: mean=69.8, std=9.8\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-3.png){width=662 height=470}\n:::\n:::\n\n\n* In an ideal scenario a feature/variable such as `weight` might be transformed in the following way after normalization:\n\n::: {#05e59e2f .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-1.png){width=971 height=395}\n:::\n:::\n\n\n* And here is what it might look like for a feature such as `age`.\n\n::: {#bc5a754f .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nZ-scored mean: -0.00, std: 1.00\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-4-output-2.png){width=971 height=395}\n:::\n:::\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* After normalization, the *normalized features* are on comparable scales. The features (such as `weight` and `age`) no longer have so much variation. They can be used as input to machine learning algorithms.\n\n* The rule of thumb is to (almost) always *normalize* your data before you use it in a machine learning algorithm. (There are a few exceptions and we will point this out in due course).\n\n:::\n<!-- end callout -->\n\n\n\n\n\n\n\n### Data visualization before doing PCA {#sec-datavizbeforePCA}\n\n::::: {#ex-titledaatviz .callout-exercise}\n\n#### exercise_data_visualization\n\n{{< level 1 >}}\n\nDiscuss in a group. What is wrong with the following plot?\n\n::: {#d7e06687 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-5-output-1.png){width=971 height=395}\n:::\n:::\n\n\n:::: {.callout-answer collapse=\"true\"}\n\n#### Looking at your data\n\nAlways look at your data before you try and machine learning technique on it. There is a 150 year old person in your data!\n\n\n\n::::\n\n:::::\n\n\n\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* Visualize your data before you do any normalization. If there is anything odd about your data, discuss this with the person who gave you the data or did the experiment. This could be an error in the machine that generated the data or a data entry error. If there is justification, you can remove the data point.\n\n* Then perform normalization and apply a machine learning technique.\n\n:::\n<!-- end callout -->\n\n\n\n\n## Setup\n\n::: {#33fc7b02 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n```\n:::\n\n\n## Example Data\n\n::: {#5d93efc5 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-7-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## PCA Example\n\n<!--open tab-->\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#392a5039 .cell execution_count=7}\n``` {.python .cell-code}\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.title(\"PCA Projection\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A simple PCA plot](normalization_files/figure-html/cell-8-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n\n\n\n## Scree plot\n\nA *scree* plot is a simple graph that shows how much variance (information) each principal component explains in your data after running PCA. The x-axis shows the principal components (PC1, PC2, etc.), and the y-axis shows the proportion of variance explained by each one.\n\nYou can use a scree plot to decide how many principal components to keep: look for the point where the plot levels off (the *elbow*): this tells you that adding more components doesn‚Äôt explain much more variance.\n\n::: {#435617b3 .cell execution_count=8}\n``` {.python .cell-code}\n# Scree plot: variance explained by each component\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Variance Explained Ratio\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-9-output-1.png){width=589 height=449}\n:::\n:::\n\n\nA scree plot may have an *elbow* like the plot below.\n\n::: {#4cb5cc71 .cell fig.cap='An idealized scree plot' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-10-output-1.png){width=663 height=449}\n:::\n:::\n\n\n<!--\n## Loadings\n\n::: {#73d3d48e .cell execution_count=10}\n``` {.python .cell-code}\n# pca.components_.T\n#feature_names = [\"Feature 1\", \"Feature 2\"]  # Replace with your actual feature names if available\n#loadings = pd.DataFrame(pca.components_.T, columns=[\"PC1\", \"PC2\"], index=feature_names)\n#print(\"PCA Loadings:\")\n#print(loadings)\n```\n:::\n\n\n-->\n\n\n\n\n### Hands-on coding\n\n* Perform PCA on a dataset of US Arrests\n\n\n* Simple method first\n\n::: {#c2c027cd .cell execution_count=11}\n``` {.python .cell-code}\n!pip install pandas numpy scikit-learn seaborn matplotlib\n```\n:::\n\n\n* Load libraries and data\n\n::: {#050a67f4 .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the US Arrests data\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\nX = pd.read_csv(url, index_col=0)\n\n# what is in the data?\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Murder</th>\n      <th>Assault</th>\n      <th>UrbanPop</th>\n      <th>Rape</th>\n    </tr>\n    <tr>\n      <th>State</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Alabama</th>\n      <td>13.2</td>\n      <td>236</td>\n      <td>58</td>\n      <td>21.2</td>\n    </tr>\n    <tr>\n      <th>Alaska</th>\n      <td>10.0</td>\n      <td>263</td>\n      <td>48</td>\n      <td>44.5</td>\n    </tr>\n    <tr>\n      <th>Arizona</th>\n      <td>8.1</td>\n      <td>294</td>\n      <td>80</td>\n      <td>31.0</td>\n    </tr>\n    <tr>\n      <th>Arkansas</th>\n      <td>8.8</td>\n      <td>190</td>\n      <td>50</td>\n      <td>19.5</td>\n    </tr>\n    <tr>\n      <th>California</th>\n      <td>9.0</td>\n      <td>276</td>\n      <td>91</td>\n      <td>40.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n* Normalize the data\n\n::: {#3434be89 .cell execution_count=13}\n``` {.python .cell-code}\nscaler_standard = StandardScaler()\nX_scaled = scaler_standard.fit_transform(X)\n```\n:::\n\n\n* Perform PCA\n\n::: {#1f287cb5 .cell execution_count=14}\n``` {.python .cell-code}\npca_fn = PCA()\nX_pca = pca_fn.fit_transform(X_scaled)\n```\n:::\n\n\n* Plotting\n\n::: {#dd9b7b22 .cell execution_count=15}\n``` {.python .cell-code}\nplt.figure()\nplt.scatter(X_pca[:,0], X_pca[:,1])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA on crime data\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-16-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* Label the plot by US State\n\n::: {#0921a27a .cell execution_count=16}\n``` {.python .cell-code}\n# States come from the index\nX.index\n\nstates = X.index # fetch states and assign it to a variable\n\n# map each state to a code\ncolour_codes_states = pd.Categorical(states).codes\n\n# pd.Categorical(states): Converts the sequence states (e.g., a list/Index of state names) into a categorical type. It internally builds:\n# categories: the unique labels (e.g., all distinct state names)\n# codes: integer labels pointing to those categories\n\nplt.figure()\nplt.scatter(X_pca[:,0], X_pca[:,1], c = colour_codes_states)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA on crime data (coloured by US state)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-17-output-1.png){width=600 height=449}\n:::\n:::\n\n\n<!-- method to have labels -->\n\n\n\n<!--* Get the loadings -->\n\n\n\n* Another method using the `pca` package; prettier plots\n\nInstall the `pca` Python package\n\n\n```python\n!pip install pca\n```\n\n\n\n* Load data\n\n::: {#7c9c83f3 .cell execution_count=20}\n``` {.python .cell-code}\nfrom pca import pca\nimport pandas as pd\n\n# Load the US Arrests data\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\ndf = pd.read_csv(url, index_col=0)\n\nprint(\"US Arrests Data (first 5 rows):\")\nprint(df.head())\nprint(\"\\nData shape:\", df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUS Arrests Data (first 5 rows):\n            Murder  Assault  UrbanPop  Rape\nState                                      \nAlabama       13.2      236        58  21.2\nAlaska        10.0      263        48  44.5\nArizona        8.1      294        80  31.0\nArkansas       8.8      190        50  19.5\nCalifornia     9.0      276        91  40.6\n\nData shape: (48, 4)\n```\n:::\n:::\n\n\n<!-- code below is not used -->\n\n\n\n* Normalize the data and perform PCA\n\n::: {#ed641df0 .cell execution_count=22}\n``` {.python .cell-code}\nmodel = pca(normalize=True)\nout = model.fit_transform(df)\nax = model.biplot()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-23-output-1.png){width=2067 height=1271}\n:::\n:::\n\n\n* Variance explained plots\n\n::: {#477c339b .cell execution_count=23}\n``` {.python .cell-code}\nmodel.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n(<Figure size 1440x960 with 1 Axes>,\n <Axes: title={'center': 'Cumulative explained variance\\n 3 Principal Components explain [100.0%] of the variance.'}, xlabel='Principal Component', ylabel='Percentage explained variance'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-24-output-2.png){width=1212 height=875}\n:::\n:::\n\n\n* 3D PCA biplots\n\n::: {#984b56e0 .cell execution_count=24}\n``` {.python .cell-code}\nmodel.biplot3d()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n(<Figure size 3000x2500 with 1 Axes>,\n <Axes3D: title={'center': '3 Principal Components explain [100.0%] of the variance'}, xlabel='PC1 (61.6% expl.var)', ylabel='PC2 (24.7% expl.var)', zlabel='PC3 (9.14% expl.var)'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-25-output-2.png){width=1945 height=1972}\n:::\n:::\n\n\n* Loadings\n\n*Recall*\n\nWhat is being plotted on the axes (PC1 and PC2) are the `scores`.\n\nThe `scores` for each principal component are calculated as follows:\n\n$$\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + .... \n$$\n\nwhere $X$, $Y$ and $Z$ are the normalized *features*.\n\nThe constants $\\alpha$, $\\beta$, $\\gamma$ are determined by the PCA algorithm. They are called the `loadings`.\n\n::: {#18de53b5 .cell execution_count=25}\n``` {.python .cell-code}\nprint(model.results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'loadings':        Murder   Assault  UrbanPop      Rape\nPC1  0.533785  0.583489  0.284213  0.542068\nPC2 -0.428765 -0.190485  0.865950  0.173225\nPC3 -0.331927 -0.267593 -0.386784  0.817690, 'PC':                      PC1       PC2       PC3\nAlabama         0.923886 -1.127792 -0.437720\nAlaska          1.884005 -1.032585  2.032973\nArizona         1.705462  0.730059  0.043498\nArkansas       -0.198714 -1.092074  0.111217\nCalifornia      2.462479  1.513698  0.585558\nColorado        1.453427  0.982671  1.080932\nConnecticut    -1.406810  1.081895 -0.661238\nDelaware       -0.003621  0.319738 -0.730442\nFlorida         2.947649 -0.070435 -0.569823\nGeorgia         1.571384 -1.281416 -0.326932\nHawaii         -0.966398  1.557165  0.034386\nIdaho          -1.689257 -0.178154  0.241665\nIllinois        1.320695  0.653978 -0.681444\nIndiana        -0.561650  0.161720  0.218372\nIowa           -2.302281  0.133259  0.145716\nKansas         -0.850716  0.279295  0.013602\nKentucky       -0.808869 -0.934920 -0.029023\nLouisiana       1.500981 -0.882536 -0.772483\nMaine          -2.444195 -0.340245 -0.083049\nMaryland        1.702710 -0.431039 -0.158134\nMassachusetts  -0.536401  1.454143 -0.626920\nMichigan        2.044350  0.144860  0.383014\nMinnesota      -1.742422  0.647555  0.133541\nMississippi     0.932617 -2.374555 -0.724196\nMissouri        0.637255  0.263934  0.369919\nMontana        -1.239466 -0.507562  0.236769\nNebraska       -1.317489  0.212450  0.160150\nNevada          2.806905  0.760007  1.157898\nNew Hampshire  -2.431886  0.048021  0.018380\nNew Jersey      0.127587  1.417883 -0.775421\nNew Mexico      1.917815 -0.148279  0.181459\nNew York        1.623118  0.790157 -0.646164\nNorth Carolina  1.064086 -2.207350 -0.854340\nNorth Dakota   -3.038797 -0.548177  0.281399\nOhio           -0.281823  0.736114 -0.041732\nOklahoma       -0.366423  0.292555 -0.026415\nOregon          0.003276  0.556212  0.921912\nPennsylvania   -0.941353  0.568486 -0.411608\nRhode Island   -0.909909  1.464948 -1.387731\nSouth Carolina  1.257310 -1.914756 -0.290121\nSouth Dakota   -2.038884 -0.778125  0.375435\nTennessee       0.935690 -0.851392  0.192734\nTexas           1.293269  0.387317 -0.490484\nUtah           -0.602262  1.466342  0.271830\nVermont        -2.851337 -1.332665  0.825094\nVirginia       -0.153441 -0.190521  0.005751\nWashington     -0.270617  0.975724  0.604878\nWest Virginia  -2.160933 -1.375609  0.097337, 'explained_var': array([0.61629429, 0.86387677, 0.95532444, 1.        ]), 'variance_ratio': array([0.61629429, 0.24758248, 0.09144767, 0.04467556]), 'model': PCA(n_components=np.int64(3)), 'scaler': StandardScaler(), 'pcp': np.float64(1.0000000000000002), 'topfeat':     PC   feature   loading  type\n0  PC1   Assault  0.583489  best\n1  PC2  UrbanPop  0.865950  best\n2  PC3      Rape  0.817690  best\n3  PC1    Murder  0.533785  weak, 'outliers':                  y_proba     p_raw    y_score  y_bool  y_bool_spe  y_score_spe\nAlabama         0.883607  0.572223   4.780770   False       False     1.457903\nAlaska          0.771864  0.061516  12.020355   False       False     2.148419\nArizona         0.883607  0.487777   5.447889   False       False     1.855152\nArkansas        0.994964  0.849865   2.662427   False       False     1.110005\nCalifornia      0.771864  0.073767  11.512662   False       False     2.890516\nColorado        0.883607  0.291940   7.323773   False       False     1.754449\nConnecticut     0.883607  0.376298   6.434671   False       False     1.774715\nDelaware        0.997194  0.934870   1.827387   False       False     0.319759\nFlorida         0.827974  0.105185  10.498050   False       False     2.948491\nGeorgia         0.883607  0.334106   6.858767   False       False     2.027628\nHawaii          0.883607  0.482129   5.494442   False       False     1.832672\nIdaho           0.883607  0.589071   4.652640   False       False     1.698626\nIllinois        0.883607  0.518417   5.200102   False       False     1.473744\nIndiana         0.997522  0.957121   1.535211   False       False     0.584469\nIowa            0.883607  0.341171   6.785183   False       False     2.306135\nKansas          0.997194  0.915334   2.046914   False       False     0.895390\nKentucky        0.942972  0.766164   3.332046   False       False     1.236262\nLouisiana       0.883607  0.372576   6.470687   False       False     1.741210\nMaine           0.883607  0.264681   7.652531   False       False     2.467763\nMaryland        0.883607  0.543590   5.001739   False       False     1.756421\nMassachusetts   0.883607  0.512543   5.247023   False       False     1.549922\nMichigan        0.883607  0.406682   6.149239   False       False     2.049476\nMinnesota       0.883607  0.477475   5.533020   False       False     1.858861\nMississippi     0.827974  0.134373   9.776761   False       False     2.551134\nMissouri        0.997194  0.908446   2.118885   False       False     0.689750\nMontana         0.942972  0.701130   3.819185   False       False     1.339364\nNebraska        0.942972  0.758317   3.391711   False       False     1.334509\nNevada          0.771864  0.052403  12.462954   False       False     2.907976\nNew Hampshire   0.883607  0.317979   7.031119   False       False     2.432360\nNew Jersey      0.883607  0.577856   4.737795   False       False     1.423612\nNew Mexico      0.883607  0.502690   5.326337   False       False     1.923539\nNew York        0.883607  0.382425   6.375898   False       False     1.805231\nNorth Carolina  0.827974  0.137996   9.697217   False       False     2.450444\nNorth Dakota    0.771864  0.080403  11.269266   False       False     3.087845\nOhio            0.997194  0.934716   1.829224   False       False     0.788218\nOklahoma        0.997522  0.982272   1.082928   False       False     0.468886\nOregon          0.994964  0.843369   2.717557   False       False     0.556221\nPennsylvania    0.942972  0.749878   3.455517   False       False     1.099691\nRhode Island    0.883607  0.234460   8.050041   False       False     1.724530\nSouth Carolina  0.883607  0.243404   7.928295   False       False     2.290659\nSouth Dakota    0.883607  0.291991   7.323178   False       False     2.182321\nTennessee       0.942972  0.719456   3.683208   False       False     1.265063\nTexas           0.942972  0.649265   4.202711   False       False     1.350022\nUtah            0.883607  0.576700   4.746601   False       False     1.585206\nVermont         0.771864  0.038043  13.332939   False       False     3.147398\nVirginia        0.997522  0.997522   0.524914   False       False     0.244627\nWashington      0.942972  0.761722   3.365861   False       False     1.012557\nWest Virginia   0.883607  0.177782   8.926009   False       False     2.561626, 'outliers_params': {'paramT2': (np.float64(-2.4671622769447922e-17), np.float64(1.273765915366402)), 'paramSPE': (array([-9.25185854e-17, -1.38777878e-17]), array([[2.51762774e+00, 6.29946348e-17],\n       [6.29946348e-17, 1.01140077e+00]]))}}\n```\n:::\n:::\n\n\n## Exercise for normalization in PCA {#sec-pcanorm}\n\n::::: {#ex-title_pca .callout-exercise}\n\n#### exercise_pca_normalization\n\n{{< level 2 >}}\n\nWork in a group.\n\n* Try the same code above but now *without* normalisation.\n\n* What differences do you observe in PCA *with* and *without* normalization?\n\n\n:::::\n\n\n\n\n\n## Exercise (advanced)\n\nPlot prettier *publication ready* plots for PCA.\n\n::: {.callout-tip}\nLook into the documentation available here for the [PCA package](https://erdogant.github.io/pca/pages/html/Examples.html).\n:::\n<!-- end callout -->\n\n\n\n\n## Exercise (theoretical) {#sec-ex-theoretical}\n\n::::: {#ex-titletheor .callout-exercise}\n\n#### exercise_theoretical\n\n{{< level 2 >}}\n\nBreak up into groups and discuss the following problem:\n\n1. Shown are biological samples with scores\n\n2. The features are genes\n\n* Why are `Sample 33` and `Sample 24` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Why is `Sample 2` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Can we treat `Sample 2` as an outlier? Why or why not? Argue your case.\n\nThe PCA biplot is shown below:\n\n::: {#8c4e4c91 .cell execution_count=26}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-27-output-1.png){width=737 height=702}\n:::\n:::\n\n\nThe table of loadings is shown below:\n\n::: {#024e7a2c .cell execution_count=27}\n\n::: {.cell-output .cell-output-stdout}\n```\n            PC1       PC2       PC3       PC4\nGene1 -0.535899  0.418181 -0.341233  0.649228\nGene2 -0.583184  0.187986 -0.268148 -0.743075\nGene3 -0.278191 -0.872806 -0.378016  0.133877\nGene4 -0.543432 -0.167319  0.817778  0.089024\n```\n:::\n:::\n\n\n:::::\n<!-- end callout -->\n\n\n\n\n<!--\n## Clustering Example\n\nPCA is different to clustering where you are trying to find patterns in your data. We will encounter clustering later in the course.\n\n\n\n-->\n\n\n## üß† PCA vs. Other Techniques\n\n* PCA is **unsupervised** (no labels used)\n* Works best for **linear** relationships\n* Alternatives:\n\n  * t-SNE for nonlinear structures\n\n---\n\n## üß¨ In Practice: Tips for Biologists\n\n* Always **standardize** data before PCA\n* Be cautious interpreting PCs biologically‚ÄîPCs are **mathematical constructs**\n\n\n\n### Goals of unsupervised learning\n\n* Finding patterns in data\n\nHere is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].\n\n![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)\n\n![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)\n\n\n* Finding interesting patterns\n\nYou can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.\n\n::: {#c878bfa5 .cell execution_count=29}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-30-output-1.png){width=616 height=481}\n:::\n:::\n\n\n* Finding outliers\n\nYou can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.\n\n::: {#2387a5fc .cell execution_count=30}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-31-output-1.png){width=657 height=481}\n:::\n:::\n\n\n* Finding hypotheses\n\nAll of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.\n\n\n## Though exercise\n\n* Can you think of a technique where unsupervised learning gets used a lot? _Hint_: we use it almost every day now! (or atleast I do)\n\n::: {.callout-hint collapse=‚Äùtrue‚Äù}\n\nChatGPT or Generative AI. The first step in processing the huge amount of text is to *reduce* the dimensions of the data using something similar to PCA.\n\n:::\n\n\n\n\n\n\n::: {.callout-tip}\n## Summary\n\n- Need to normalize data before doing dimensionality reduction\n- PCA reduces dimensionality for visualization.\n- Clustering algorithms finds clusters in unlabeled data.\n- The goal of unsupervised learning is to find patterns and form hypotheses.\n:::\n\n\n## Resources\n\n[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)\n\n[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)\n\n[3] [ISLP book](https://www.statlearning.com/)\n\n[4] [Video lectures by the authors of the book Introduction to Statistical Learning in Python](https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda)\n\n[6] [Visual explanations of machine learning algorithms](https://mlu-explain.github.io)\n\n---\n\n",
    "supporting": [
      "normalization_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}