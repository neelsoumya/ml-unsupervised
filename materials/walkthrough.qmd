---
title: "Introduction to Unsupervised Learning"
format: html
---

::: {.callout-tip}
## Learning Objectives

By the end of this module, learners will be able to:

* Define unsupervised learning and explain how it differs from supervised learning in terms of inputs, outputs, and goals.

* Identify common unsupervised techniques, including clustering (e.g., k‚Äëmeans, hierarchical) and dimensionality reduction (e.g., PCA), and describe when each is appropriate.

* Discuss real‚Äëworld applications of unsupervised learning, such as customer segmentation, anomaly detection, and image compression.

* Explain the role of unsupervised learning in exploratory data analysis.

* Interpret principal component analysis (PCA) intuitively to understand how PCA finds the directions of greatest variance in data.

* Apply dimensionality reduction to a simple multivariate dataset (e.g., crime rates and population by state) to visualize high‚Äëdimensional data in two or three dimensions.

* Differentiate unsupervised from supervised problems by examining datasets and deciding whether the task is to uncover patterns versus predict a known target variable.

* Articulate the value of unsupervised learning in uncovering hidden structure in unlabelled data and its importance as data complexity grow.
:::



## Introduction

Unsupervised learning is a branch of machine learning that deals with finding hidden patterns or intrinsic structures in data without the use of labeled responses. Unlike supervised learning, where the model learns from labeled data to predict outcomes, unsupervised learning works with input data that does not have any corresponding output variables. The primary goal is to explore the underlying structure, groupings, or features in the data.

One of the most common applications of unsupervised learning is clustering, where the algorithm groups similar data points together based on their characteristics. This is particularly useful in scenarios such as customer segmentation, anomaly detection, and image compression. Another key technique is dimensionality reduction, which aims to reduce the number of variables under consideration, making it easier to visualize and interpret large datasets.

Unsupervised learning is valuable because it can reveal insights that may not be immediately apparent, uncovering relationships and patterns that might otherwise go unnoticed. It is commonly used in exploratory data analysis and as a preprocessing step for other algorithms. As data continues to grow in complexity and volume, unsupervised learning plays a critical role in making sense of unstructured information.

### Motivation

Here is a picture I took of a pavement in Cambridge the day after Valentine's Day. Why did this picture capture my attention? The starkness of the grey pavement contrasted with the bright red rose. It may have triggered some unsupervised learning mechanism in my brain that allows me to pick anomalies!

![Rose after Valentine's Day](images/rose_after_valentines_day.png)

Unsupervised learning is all about discovering structure in data without any explicit ‚Äúright answers‚Äù to guide you. Your rose‚Äëon‚Äëpavement photo is a perfect real‚Äëworld illustration of a few core ideas:

* Anomaly (or Outlier) Detection
- **What happened in your brain:**  
  When you look at a uniform grey pavement, your visual system builds an internal ‚Äúmodel‚Äù of what‚Äôs normal‚Äîflat, texture‚Äërepeating, monochrome. The bright red rose doesn‚Äôt fit that model, so it ‚Äúpops,‚Äù drawing your attention.

- **In machine learning:**  
  Algorithms like‚ÄØIsolation Forests, One‚ÄëClass SVMs, or autoencoder‚Äëbased detectors learn a representation of ‚Äúnormal‚Äù data (e.g. patches of pavement) and then flag anything that deviates significantly (e.g. the rose) as an anomaly.

* Feature Extraction & Saliency
- **Human vision analogy:**  
  Early in the visual cortex, neurons respond to edges, color contrasts, textures. A red circle on grey evokes strong responses in ‚Äúcolor‚Äù and ‚Äúshape‚Äëedge‚Äù channels.

- **ML counterpart:**  
  Techniques like PCA or deep autoencoders learn low‚Äëdimensional ‚Äúfeatures‚Äù (color histograms, texture filters). Dimensions where the rose is extreme (high red‚Äëchannel value) are exactly the ones that give us the ‚Äúanomaly‚Äù score.

* Clustering & Pattern Discovery
You might not only notice the rose, but if there were lots of petals scattered around, your brain could start grouping (clustering) regions of similar color/shape.

Unsupervised clustering algorithms (k‚Äëmeans, DBSCAN) would partition image patches into clusters‚Äî‚Äúpavement patches,‚Äù ‚Äúrose petals,‚Äù maybe even ‚Äúshadows.‚Äù Anything that doesn‚Äôt belong to a big cluster may again be flagged as rare.

* Dimensionality Reduction & Visualization
In a high‚Äëdimensional feature space (e.g. each 10√ó10 pixel patch ‚Üí a 300‚Äëdim vector), you can‚Äôt ‚Äúsee‚Äù clusters easily. Algorithms like t‚ÄëSNE or UMAP compress that down to 2D so you can actually plot and see the rose‚Äëpatches separate from pavement.

This is why, for instance, visual analytics tools will show outliers as distant points on a scatterplot‚Äîjust as you instantly spot the rose on the pavement.



### Resources

[PCA intuition](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)


::: {.callout-tip}
### Key Concept 

![Information bottleneck](images/information_bottleneck.png)

In unsupervised learning, the __bottleneck concept__ refers to a deliberate architectural constraint in a model‚Äîtypically an autoencoder‚Äîwhere information is compressed through a narrow intermediate representation, often called a latent code or embedding. The model is trained to reconstruct the input data after passing it through this low-dimensional bottleneck, forcing it to learn a compact and informative representation of the underlying structure of the data. Since there are no labels guiding the learning process, the model relies solely on reconstructing its input as accurately as possible, using only the limited information passed through this narrow channel. This compression encourages the model to capture essential features while discarding noise or redundancy.

The bottleneck acts as an inductive bias that promotes dimensionality reduction, feature learning, and denoising. By minimizing reconstruction error while constrained by a reduced latent space, the model implicitly discovers patterns, clusters, and hierarchies within the input data. In practical terms, this is a foundational principle behind many unsupervised representation learning methods, including classical autoencoders, variational autoencoders (VAEs), and self-supervised learning systems that rely on contrastive or generative objectives. The learned low-dimensional codes can then be used for downstream tasks such as clustering, visualization (e.g., with t-SNE or PCA), or as inputs to supervised models in a semi-supervised setting.

magine you have a huge library of biological images‚Äîsay, pictures of different cell types under a microscope‚Äîand you want to teach a computer to recognize patterns in those images without telling it what any of the cells are. A ‚Äúbottleneck‚Äù in this context is like asking the computer to summarize each image using only a few key words instead of the entire picture. By forcing it to compress all the rich detail down to a small summary, the computer has to figure out which features‚Äîlike cell shape, size, or texture‚Äîare truly important. This is similar to how a biologist might sketch a simplified diagram of a cell, highlighting its nucleus and membrane but leaving out every ribosome and microtubule.

Because the computer must recreate the original image from that stripped‚Äëdown summary, it learns to ignore random noise or unimportant quirks (like slight variations in lighting) and focus on the core characteristics shared by similar cell types. In other words, the bottleneck helps the machine discover the hidden ‚Äúessence‚Äù of the data. Once you have those concise summaries, you can use them to cluster cells into groups, visualize how different cell types relate, or even feed them into a second analysis‚Äîjust as you might reduce a complex DNA dataset to a handful of genetic markers before drawing a phylogenetic tree. This approach lets you explore and interpret large biological datasets more effectively, all without ever providing explicit labels.
:::


### Example

Given the data below, how should we reduce the number of features and/or visualize it? This is an **unsupervised** machine learning problem.

::: {.callout-tip}
**NOTE (IMPORTANT CONCEPT)**: The columns of this data are the *features*.
:::


| State       | Murder (per 100k) | Robbery (per 100k) | Population     |
|-------------|-------------------|--------------------|----------------|
| California  | 9.1               | 45.3               | 39,512,223     |
| Texas       | 7.8               | 38.6               | 28,995,881     |
| Florida     | 5.9               | 31.7               | 21,477,737     |
| New York    | 3.4               | 26.4               | 19,453,561     |
| Illinois    | 6.4               | 35.1               | 12,671,821     |
| Pennsylvania| 4.8               | 22.9               | 12,801,989     |


Importantly, we are not trying to predict anything. For example, say in the data below we can try to predict the number of people who moved to that state last year. This is a **supervised** machine learning problem [@Gareth2017].

| State        | Murder (per 100k) | Robbery (per 100k) | Population   | People Who Moved (per 100k) |
|--------------|-------------------|--------------------|--------------|-----------------------------|
| California   | 9.1               | 45.3               | 39,512,223   | 5,400                       |
| Texas        | 7.8               | 38.6               | 28,995,881   | 4,100                       |
| Florida      | 5.9               | 31.7               | 21,477,737   | 6,200                       |
| New York     | 3.4               | 26.4               | 19,453,561   | 3,800                       |
| Illinois     | 6.4               | 35.1               | 12,671,821   | 2,900                       |
| Pennsylvania | 4.8               | 22.9               | 12,801,989   | 2,500                       |


## What PCA does to the data

<!--
[PCA in 3D](https://github.com/neelsoumya/python_machine_learning/blob/main/pca_intro_3D_view.ipynb)
-->



### Projection of 3D Data

We generate three clusters of synthetic 3‚Äëdimensional points, compute the first two principal components using scikit‚Äëlearn‚Äôs PCA, and then create a two‚Äëpanel figure:

1. **Left panel**: A 3D scatter of the original points, the best‚Äêfit plane defined by the first two principal components, and projection lines from each point down onto that plane.  
2. **Right panel**: A 2D scatter of the projected coordinates (the principal component scores) along the first two components, colored by cluster.

Use this visualization to understand how PCA finds the plane that maximizes variance and how the data look when reduced to two dimensions.

```{python ch1-import-3d-pca}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D
```

```{python ch1-3d-pca}
#| echo: false


import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 1. Generate synthetic 3D data (three clusters)
np.random.seed(0)
n = 30
means = [(-2, 0, 0), (2, 0, 0), (0, 2, 2)]
data = np.vstack([np.random.randn(n, 3) + m for m in means])
labels = np.array([0]*n + [1]*n + [2]*n)

# 2. Perform PCA to get first two components
pca = PCA(n_components=2)
scores = pca.fit_transform(data)
components = pca.components_
mean_3d = pca.mean_

# 3. Create a meshgrid for the principal component plane
grid_x, grid_y = np.meshgrid(
    np.linspace(-4, 4, 10),
    np.linspace(-4, 4, 10)
)
# Plane points: mean + x*pc1 + y*pc2
plane_points = mean_3d + grid_x[..., None] * components[0] + grid_y[..., None] * components[1]

# 4. Inverse transform scores to get projections back into 3D
proj_3d = pca.inverse_transform(scores)

# 5. Plotting with vertical layout (2 rows, 1 column)
fig = plt.figure(figsize=(8, 10))

# Top: 3D scatter with PCA plane and projection lines
ax1 = fig.add_subplot(2, 1, 1, projection='3d')
ax1.scatter(data[:, 0], data[:, 1], data[:, 2], c=labels)
ax1.plot_surface(
    plane_points[:, :, 0],
    plane_points[:, :, 1],
    plane_points[:, :, 2],
    alpha=0.2,
    edgecolor='gray',
    linewidth=0.5
)
for i in range(data.shape[0]):
    xs = [data[i, 0], proj_3d[i, 0]]
    ys = [data[i, 1], proj_3d[i, 1]]
    zs = [data[i, 2], proj_3d[i, 2]]
    ax1.plot(xs, ys, zs, linewidth=0.5)
ax1.set_title('Data and first two principal components')
ax1.set_xlabel('X‚ÇÅ')
ax1.set_ylabel('X‚ÇÇ')
ax1.set_zlabel('X‚ÇÉ')

# Bottom: 2D scatter of PCA scores
ax2 = fig.add_subplot(2, 1, 2)
ax2.scatter(scores[:, 0], scores[:, 1], c=labels)
ax2.set_title('Projection onto first two PCs')
ax2.set_xlabel('First principal component')
ax2.set_ylabel('Second principal component')

plt.tight_layout()
plt.show()

```


## PCA is lossy 

PCA does lose some information. But it can capture some/most of the salient aspects of your data. 


::: {.callout-tip}
**NOTE (IMPORTANT CONCEPT)** 

Dimensionality reduction techniques (such as PCA) always lose some information. In other words, it is *lossy*.
:::


### Lesson on lossy compression (PCA applied to image)

### Learning Objectives

* Understand how Principal Component Analysis (PCA) can be applied to images.
* Observe how PCA captures the most significant patterns in image data.
* Visualize how the number of principal components affects image reconstruction.
* Appreciate the trade-off between compression and information loss.

### Key Concepts

* **PCA** is a dimensionality reduction technique that identifies directions (principal components) along which the variance in the data is maximized.
* Images can be viewed as high-dimensional data (each pixel as a feature), and PCA helps reduce that dimensionality while preserving key patterns.

### Procedure Overview

1. **Load and display an image** from a URL.
2. **Apply PCA to each RGB channel** of the image separately.
3. **Reconstruct the image** using an increasing number of principal components.
4. **Visualize the reconstructions** to show how few components capture most of the image's structure.

```{python ch1-pca-lossy-image}

#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import requests
from PIL import Image
from io import BytesIO

# Load the image from GitHub
url = "https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/images/rose_after_valentines_day.png"
response = requests.get(url)
img = Image.open(BytesIO(response.content)).convert("RGB")

# Resize for computational ease (optional)
img = img.resize((img.width // 4, img.height // 4))
img_np = np.array(img)

# Display original image
plt.figure(figsize=(5, 5))
plt.title("Original Image")
plt.imshow(img_np)
plt.axis("off")
plt.show()

# Function to apply PCA to each color channel
def pca_on_channel(channel_data, n_components):
    original_shape = channel_data.shape
    flat_data = channel_data.reshape(-1, 1)
    pca = PCA(n_components=n_components)
    reduced = pca.fit_transform(flat_data)
    reconstructed = pca.inverse_transform(reduced)
    return reconstructed.reshape(original_shape)

# Apply PCA to each channel separately with varying components
components_list = [1, 5, 10, 20, 50]
fig, axs = plt.subplots(1, len(components_list), figsize=(15, 5))

for i, n_components in enumerate(components_list):
    # Flatten each channel for PCA
    reconstructed_channels = []
    for c in range(3):  # RGB
        channel = img_np[:, :, c]
        h, w = channel.shape
        pca = PCA(n_components=n_components)
        reduced = pca.fit_transform(channel)
        reconstructed = pca.inverse_transform(reduced)
        reconstructed_channels.append(reconstructed)

    reconstructed_img = np.stack(reconstructed_channels, axis=2).astype(np.uint8)
    axs[i].imshow(reconstructed_img)
    axs[i].set_title(f"{n_components} PC")
    axs[i].axis("off")

plt.suptitle("Image Reconstruction with Varying Principal Components", fontsize=14)
plt.tight_layout()
plt.show()

```

### Takeaway Message

PCA can significantly reduce image data dimensionality while preserving salient features, making it a powerful tool for image compression and understanding. However, perfect reconstruction is only possible with all components, revealing the balance between efficiency and fidelity.


::: {.callout-tip}
## Activity: Playable version of PCA in browser

[PCA in your browser](https://projector.tensorflow.org/)
:::


## Visual explanations of PCA

```{python ch1-visual-explanation}
#| echo: false
#| fig.cap: "Visual explanation of PCA"

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Generate synthetic 2D data
np.random.seed(42)
X = np.dot(np.random.rand(2, 2), np.random.randn(2, 100)).T

# Fit PCA
pca = PCA(n_components=2)
pca.fit(X)
X_pca = pca.transform(X)

# Get principal component vectors
origin = np.mean(X, axis=0)
pc1 = pca.components_[0] * np.sqrt(pca.explained_variance_[0]) * 3
pc2 = pca.components_[1] * np.sqrt(pca.explained_variance_[1]) * 3

plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], alpha=0.5, label='Original Data')
plt.quiver(*origin, *pc1, color='r', scale=1, scale_units='xy', angles='xy', width=0.01, label='PC1')
plt.quiver(*origin, *pc2, color='b', scale=1, scale_units='xy', angles='xy', width=0.01, label='PC2')

# Project points onto PC1
for point in X:
    proj = origin + np.dot((point - origin), pca.components_[0]) * pca.components_[0]
    plt.plot([point[0], proj[0]], [point[1], proj[1]], 'g--', linewidth=0.5, alpha=0.7)
plt.title('PCA: Principal Components and Projections')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.axis('equal')
plt.grid(True)
plt.show()

```


::: {.callout-tip}
## Intuition

* PCA maximimizes the variance captured

```{python ch1-pca-intuition-max-variance}
#| echo: false
#| fig.cap: "Intuition behind PCA"

import numpy as np
import matplotlib.pyplot as plt

# Generate random 2D data
np.random.seed(42)
mean = np.array([2, 3])
cov = np.array([[3, 1.5], [1.5, 1]])
data = np.random.multivariate_normal(mean, cov, size=100)

# Center data
data_mean = np.mean(data, axis=0)
centered_data = data - data_mean

# Compute covariance matrix and eigen decomposition
cov_matrix = np.cov(centered_data, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# Principal component (first, largest eigenvalue)
pc1 = eigenvectors[:, np.argmax(eigenvalues)]

# Project each point onto PC1
projections = np.dot(centered_data, pc1)[:, np.newaxis] * pc1
projected_points = projections + data_mean

# Plotting
plt.figure(figsize=(8, 8))
# Original data points
plt.scatter(data[:, 0], data[:, 1], color='blue', alpha=0.6, label='Original Data')
# Principal component axis line
line_points = np.array([data_mean - 4*pc1, data_mean + 4*pc1])
plt.plot(line_points[:, 0], line_points[:, 1], color='green', linewidth=2, label='PC1 Axis')
# Projected points
plt.scatter(projected_points[:, 0], projected_points[:, 1], color='red', alpha=0.6, label='Projections')

# Connect each original point to its projection
for original, proj in zip(data, projected_points):
    plt.plot([original[0], proj[0]], [original[1], proj[1]], color='gray', linestyle='--', linewidth=0.5)

plt.axis('equal')
plt.title('PCA Demonstration: Data and Projections onto 1st Principal Component')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True)
plt.show()

```


* App to explain the intuition behind PCA

![Animation](images/pca_rotating_clean.gif)

![Animation](images/pca_pendulum_animation.gif)


```{python ch1-intuition-app}
#| eval: false
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Circle

# Set random seed for reproducibility
np.random.seed(42)

# Generate correlated data
X = np.random.randn(100, 2)
correlation_matrix = np.array([[1, 0.6], [0.6, 0.6]])
L = np.linalg.cholesky(correlation_matrix)
X = X @ L.T
X = X - np.mean(X, axis=0)

# Calculate eigenvectors
cov_matrix = np.cov(X.T)
eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)

def create_static_plot():
    """Create static PCA plot"""
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 0], X[:, 1], c='blue', alpha=0.7, s=50)
    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)
    plt.grid(True, alpha=0.3)
    plt.title('PCA Data Visualization')
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.axis('equal')
    plt.xlim(-3.5, 3.5)
    plt.ylim(-3.5, 3.5)
    plt.tight_layout()
    plt.savefig('pca_static.png', dpi=300, bbox_inches='tight')
    plt.show()

def create_rotating_animation():
    """Create rotating PCA animation"""
    fig, ax = plt.subplots(figsize=(8, 6))
    
    def animate(alpha):
        ax.clear()
        ax.set_xlim(-3.5, 3.5)
        ax.set_ylim(-3.5, 3.5)
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        
        # Calculate projection direction
        w = np.array([np.cos(np.radians(alpha)), np.sin(np.radians(alpha))])
        z = X @ w.reshape(-1, 1) @ w.reshape(1, -1)
        
        # Draw projection lines
        for i in range(100):
            ax.plot([X[i, 0], z[i, 0]], [X[i, 1], z[i, 1]], 'r', alpha=0.3, linewidth=0.5)
        
        # Draw projection line
        ax.plot(w[0] * 3.5 * np.array([-1, 1]), w[1] * 3.5 * np.array([-1, 1]), 'k', linewidth=2)
        
        # Plot points
        ax.scatter(z[:, 0], z[:, 1], c='red', s=50, alpha=0.7)
        ax.scatter(X[:, 0], X[:, 1], c='blue', s=50, alpha=0.7)
        
        # Plot origin
        origin = Circle((0, 0), 0.1, color='white', ec='black', linewidth=2)
        ax.add_patch(origin)
        
        # Plot principal component direction
        pc_direction = eigenvecs[:, 1]
        ax.plot(pc_direction[0] * np.array([-4.5, -3.5]), 
                pc_direction[1] * np.array([-4.5, -3.5]), 'm', linewidth=2)
        ax.plot(pc_direction[0] * np.array([3.5, 4.5]), 
                pc_direction[1] * np.array([3.5, 4.5]), 'm', linewidth=2)
        
        ax.set_title(f'PCA Rotation Animation (Angle: {alpha}¬∞)')
        return ax,
    
    anim = animation.FuncAnimation(fig, animate, frames=range(0, 180, 3), 
                                  interval=100, blit=False, repeat=True)
    
    # Save animation
    try:
        anim.save('pca_rotating_animation.gif', writer='pillow', fps=10)
        print("Animation saved as 'pca_rotating_animation.gif'")
    except Exception as e:
        print(f"Could not save animation: {e}")
    
    plt.show()
    return anim

def create_pendulum_animation():
    """Create pendulum-style PCA animation"""
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # Initial conditions
    alpha = -45
    omega = 0
    target_angle = np.degrees(np.arctan2(eigenvecs[1, 1], eigenvecs[0, 1]))
    
    def animate_pendulum(frame):
        nonlocal alpha, omega
        
        ax.clear()
        ax.set_xlim(-3.5, 3.5)
        ax.set_ylim(-3.5, 3.5)
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
        
        # Calculate projection direction
        w = np.array([np.cos(np.radians(alpha)), np.sin(np.radians(alpha))])
        z = X @ w.reshape(-1, 1) @ w.reshape(1, -1)
        
        # Pendulum physics
        M = np.sum(np.sum((z @ np.array([[0, 1], [-1, 0]])) * (X - z), axis=1))
        omega = omega + M
        omega = omega * 0.93
        alpha = alpha + omega / 40
        
        # Draw projection lines
        for i in range(100):
            ax.plot([X[i, 0], z[i, 0]], [X[i, 1], z[i, 1]], 'r', alpha=0.3, linewidth=0.5)
        
        # Draw projection line
        ax.plot(w[0] * 3.5 * np.array([-1, 1]), w[1] * 3.5 * np.array([-1, 1]), 'k', linewidth=2)
        
        # Plot points
        ax.scatter(z[:, 0], z[:, 1], c='red', s=50, alpha=0.7)
        ax.scatter(X[:, 0], X[:, 1], c='blue', s=50, alpha=0.7)
        
        # Plot origin
        origin = Circle((0, 0), 0.1, color='white', ec='black', linewidth=2)
        ax.add_patch(origin)
        
        # Plot principal component direction
        pc_direction = eigenvecs[:, 1]
        ax.plot(pc_direction[0] * np.array([-4.5, -3.5]), 
                pc_direction[1] * np.array([-4.5, -3.5]), 'm', linewidth=2)
        ax.plot(pc_direction[0] * np.array([3.5, 4.5]), 
                pc_direction[1] * np.array([3.5, 4.5]), 'm', linewidth=2)
        
        ax.set_title(f'PCA Pendulum Animation (Angle: {alpha:.1f}¬∞, Target: {target_angle:.1f}¬∞)')
        
        # Stop animation if pendulum has settled
        if abs(omega) < 1 and abs(alpha - abs(target_angle)) < 1:
            plt.close()
            return ax,
        
        return ax,
    
    anim = animation.FuncAnimation(fig, animate_pendulum, frames=500, 
                                  interval=50, blit=False, repeat=False)
    
    # Save animation
    try:
        anim.save('pca_pendulum_animation.gif', writer='pillow', fps=20)
        print("Animation saved as 'pca_pendulum_animation.gif'")
    except Exception as e:
        print(f"Could not save animation: {e}")
    
    plt.show()
    return anim

# Create static image
print("1. Creating static image...")
create_static_plot()
    
# Create rotating animation
print("2. Creating rotating animation...")
rotating_anim = create_rotating_animation()
    
# Create pendulum animation
print("3. Creating pendulum animation...")
pendulum_anim = create_pendulum_animation()
    
  
```

:::
<!-- end callout -->


## üìä Key Concepts


### 1. **Scores and Loadings**

What is being plotted on the axes (PC1 and PC2) are the `scores`.

The `scores` for each principal component are calculated as follows:

$$
PC_{1} = \alpha X + \beta Y + \gamma Z + .... 
$$

where $X$, $Y$ and $Z$ are the normalized *features*.

The constants $\alpha$, $\beta$, $\gamma$ are determined by the PCA algorithm. They are called the `loadings`.


### 2. **Variance**

* Variance = how spread out the data is.
* PCA finds directions (principal components) that maximize variance.

Formula for variance of variable $x$:

$$
\text{Var}(x) = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$




---

## üî¨ Example: Gene Expression Data

* Rows = samples (patients)
* Columns = gene expression levels

### Goal:

* Reduce dimensionality from 20,000 genes to 2-3 PCs
* Visualize patterns between patient groups (e.g., healthy vs. cancer)

```python
# Sample Python code (requires numpy, sklearn, matplotlib)
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

X = ...  # gene expression matrix
X_scaled = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA of Gene Expression')
plt.show()
```


## Lesson Summary

* Basics of unsupervised learning

* Useful for visualization, outlier detection and making sense of your data if there are many features

- **What it is:** Discover hidden patterns or groupings in unlabeled data, without predicting a specific target.  
- **Key techniques:**  
  - **Clustering** (e.g. k‚Äëmeans, hierarchical) for grouping similar observations  
  - **Dimensionality reduction** (e.g. PCA) for compressing and visualizing high‚Äëdimensional data  
- **Why it matters:**  
  - Reveals structure in customer segmentation, anomaly detection, image compression, etc.  
  - Serves as exploratory analysis and preprocessing for downstream tasks  
- **Information bottleneck:** Forcing models (like autoencoders) to squeeze data through a narrow ‚Äúlatent code‚Äù uncovers the most essential features and removes noise  
- **Hands‚Äëon example:** Apply PCA to crime‚Äëand‚Äëpopulation data by state to project three features into two dimensions for visualization  
- **Unsupervised vs. supervised:**  
  - **Unsupervised:** No labels, focus on pattern discovery  
  - **Supervised:** With labels, focus on predicting a known outcome 




## Acknowledgements

We thank Martin van Rongen, Vicki Hodgson, Hugo Tavares, Paul Fannon, Matt Castle and the Bioinformatics Facility Training Team for their support and guidance.


## Resources

- [Introduction to Statistical Learning in Python book](https://www.statlearning.com/)

- [Video lectures by the authors of the book Introduction to Statistical Learning in Python](https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda)

- https://github.com/neelsoumya/public_teaching_unsupervised_learning

- [Interactive explanations of machine learning models](https://mlu-explain.github.io)

- [Mathematics behind unsupervised machine learning](https://mml-book.github.io/book/mml-book.pdf)



<!--
## Equation

Use LaTeX to write equations:

$$
\chi' = \sum_{i=1}^n k_i s_i^2
$$

-->